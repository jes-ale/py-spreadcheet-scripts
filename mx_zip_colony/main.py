import pandas as pd
import os
import sys
from pyexcel_ods3 import save_data
import ezodf


def generate_external_id(**kwargs):
    """
    Generates a unique external identifier based on provided keyword arguments.

    Args:
        **kwargs: Keyword arguments representing different components of the external identifier.
                  Each keyword argument should correspond to a component of the identifier.

    Returns:
        str: A unique external identifier generated by joining cleaned values of the provided keyword arguments
             with underscores.
    """
    cleaned_args = {key: value.replace(" ", "") for key, value in kwargs.items()}
    return "_".join(cleaned_args.values())


def read_file(file_name):
    """
    Reads an ODS file and returns its contents as a dictionary.

    Args:
        file_name (str): Name of the ODS file.

    Returns:
        dict: A dictionary containing each sheet name as a key and its contents as a DataFrame.
    """
    file_path = os.path.join(os.getcwd(), file_name)
    if os.path.exists(file_path):
        try:
            doc = ezodf.opendoc(file_path)
            ods_data = {}
            for sheet in doc.sheets:
                sheet_name = sheet.name
                data = []
                for row in sheet.rows():
                    row_data = []
                    for cell in row:
                        row_data.append(cell.value)
                    data.append(row_data)
                ods_data[sheet_name] = pd.DataFrame(data[1:], columns=data[0])
            return ods_data
        except Exception as e:
            print(f"Error reading ODS file '{file_name}': {e}")
            return None
    else:
        print(f"File '{file_name}' not found in the current working directory.")
        return None


def write_ods(data, output_path, sheet_name):
    """
    Writes data to an ODS file, overwriting if a file with the same name already exists.

    Args:
        data (list of dict): Data to be written to the ODS file.
        output_path (str): Absolute path for the output ODS file.
        sheet_name (str): Name of the sheet in the ODS file.

    Returns:
        None
    """
    # Remove the existing file if it exists
    if os.path.exists(output_path):
        os.remove(output_path)

    # Convert data to a dictionary suitable for saving
    sheet_data = {sheet_name: [list(data[0].keys())] + [[cell if cell is not None else '' for cell in row.values()] for row in data]}

    # Save the data to an ODS file
    save_data(output_path, sheet_data)


def read_cities(absolute_file_path, column_keys):
    """
    Reads cities (municipalities) data from the specified file and returns a dictionary grouped by state name.

    Args:
        absolute_file_path (str): Absolute path to the cities file.
        column_keys (dict): Dictionary containing keys to access columns.

    Returns:
        dict: Dictionary containing city data with state code as key.
    """
    cities = {}
    data = read_file(absolute_file_path)
    sheet_name = list(data.keys())[0]  # Assuming there's only one sheet

    for _, row in data[sheet_name].iterrows():
        city = {
            'name': row[column_keys['name']],
            'code': row[column_keys['code']],
            'external_id': row[column_keys['external_id']],
            'state_name': row[column_keys['state_name']],
            'state_external_id': row[column_keys['state_external_id']]
        }
        state_name = city['state_name']
        if state_name not in cities:
            cities[state_name] = {}
        cities[state_name][city['code']] = city
    return cities


def read_data(absolute_file_path, column_keys, cities):
    """
    Reads colonies and zip codes data from the specified file and returns dictionaries grouped by state name and city code.

    This method reads data from the provided file, which contains multiple datasets (sheets), each representing data for a specific state.
    It processes each state's data separately and returns dictionaries containing colony and zip code data, grouped by state name and city code.

    Args:
        absolute_file_path (str): Absolute path to the file.
        column_keys (dict): Dictionary containing keys to access columns.
        cities (dict): Dictionary containing city data with state code as key.

    Returns:
        dict: A nested dictionary where keys represent state names, values are dictionaries containing colony data. Each colony dictionary is keyed by city code and contains nested dictionaries for individual colonies.

        dict: A nested dictionary where keys represent state names, values are dictionaries containing zip code data. Each zip code dictionary is keyed by city code and contains nested dictionaries for individual zip codes.
    """
    colonies = {}
    zipcodes = {}

    file_data = read_file(absolute_file_path)

    for state_name, data in file_data.items():
        print(f"Read data sheet_name: {state_name}")
        if state_name not in cities:
            print(f"State name {state_name} not in cities dataset. Skipping sheet.")
            continue

        colonies[state_name] = {}
        zipcodes[state_name] = {}

        for _, row in data.iterrows():
            city_code = row[column_keys['mx_record']['city_code']]
            if city_code not in cities[state_name]:
                print(f"City code {city_code} not in cities for state {state_name}. Skipping row. {row}")
                continue

            city = cities[state_name][city_code]
            city_external_id = city['external_id']

            # Process colonies
            colony_code = row[column_keys['colony']['code']]
            colony = colonies[state_name].setdefault(city_code, {}).get(colony_code)

            if colony is None:
                colony = {
                    'name': row[column_keys['colony']['name']],
                    'code': colony_code,
                    'zip': row[column_keys['zipcode']['name']],
                    'city_code': city_code,
                    'city_external_id': city_external_id,
                    'state_name': state_name,
                    'external_id': generate_external_id(state_name=state_name, city_code=city_code, colony_code=colony_code)
                }
            else:
                colony['zip'] += f", {row[column_keys['colony']['zip']]}"

            # Add colony to output
            colonies[state_name][city_code][colony_code] = colony

            # Process zip codes
            zipcode_name = row[column_keys['zipcode']['name']]
            zipcode = zipcodes[state_name].setdefault(city_code, {}).get(zipcode_name)

            if zipcode is None:
                zipcode = {
                    'name': zipcode_name,
                    'city_code': city_code,
                    'city_external_id': city_external_id,
                    'state_name': state_name,
                    'external_id': generate_external_id(state_name=state_name, city_code=city_code, zipcode_name=zipcode_name)
                }

            # Add zipcode to output
            zipcodes[state_name][city_code][zipcode_name] = zipcode

    return colonies, zipcodes


def process_ccp_data(ccp_file_path, column_keys):
    """
    Process the CCP dataset to create a dictionary mapping zip codes to colony codes.

    Args:
        ccp (dict): Dictionary containing CCP data with sheet names as keys.
        column_keys (dict): Dictionary containing keys to access columns in the CCP dataset.

    Returns:
        dict: Dictionary mapping zip codes to colony codes.
        dict: Dictionary mapping zip codes to colony codes and names.
    """
    ccp = read_file(ccp_file_path)
    ccp_data_dict = {}
    ccp_lookup_dict = {}

    for sheet_name, data in ccp.items():
        for _, row in data.iterrows():
            zipcode_name = row[column_keys['zip_code']]
            colony_code = row[column_keys['colony_code']]
            colony_name = row[column_keys['colony_name']]
            ccp_data_dict.setdefault(zipcode_name, []).append({'zip': zipcode_name, 'code': colony_code, 'name': colony_name})
            ccp_lookup_dict.setdefault(zipcode_name, []).append(colony_code)

    return ccp_data_dict, ccp_lookup_dict


def process_directory(cities_file_path=None,
                      correos_de_mexico_file_path=None,
                      colony_output_file_path=None,
                      zip_output_file_path=None,
                      error_logs_file_path=None,
                      column_keys=None):
    """
    Processes the provided input files and generates an output file.

    Args:
        cities_file_path (str): Absolute path to the file containing cities (municipalities) data.
        correos_de_mexico_file_path (str): Absolute path to the file containing colonies data.
        output_file_path (str): Absolute path for the output file.
        column_keys (dict): Dictionary containing keys to access columns in each file.
    """
    if os.path.exists(error_logs_file_path):
        os.remove(error_logs_file_path)

    # Redirect print statements to a log file if error_logs_file_path is provided
    if error_logs_file_path:
        sys.stdout = open(error_logs_file_path, 'a')  # Append mode

    cities = read_cities(cities_file_path, column_keys['city'])
    ccp_data, ccp_lookup = process_ccp_data(ccp_file_path, column_keys['ccp'])
    colonies, zipcodes = read_data(correos_de_mexico_file_path, column_keys, cities)

    colonies_data = []

    zipcodes_data = []
    for state in zipcodes.values():
        for city in state.values():
            for zipcode in city.values():
                zipcode_name = zipcode['name']
                if zipcode_name in ccp_lookup:
                    if zipcode_name == ['22755', '22754', '22753', '22752']:
                        print(f"found  {zipcode_name} | {ccp_data[zipcode_name]} | {zipcode}")
                    zipcodes_data.append({
                        'external_id': zipcode['external_id'],
                        'name': zipcode_name,
                        'city_external_id': zipcode['city_external_id']
                    })
                    for colony in ccp_data[zipcode_name]:
                        colonies_data.append({
                            'external_id': generate_external_id(state_name=zipcode['state_name'], city_code=zipcode['city_code'], colony_code=colony['code']),
                            'name': colony['name'],
                            'code': colony['code'],
                            'city_external_id': zipcode['city_external_id'],
                            'zip_code_external_id': zipcode['external_id']
                        })

    # Write colonies and zipcodes to output file

    write_ods(colonies_data, colony_output_file_path, 'Colonies')
    write_ods(zipcodes_data, zip_output_file_path, 'Zipcodes')

    # Reset stdout to default (console) after processing
    if error_logs_file_path:
        sys.stdout.close()
        sys.stdout = sys.__stdout__


if __name__ == "__main__":
    """
    Define column names mapping to keys. Use only the keys to access the column data and use those same keys to assing the output data.
    This approach results in this structure defining the mapping from the input files column names to the output files column names. It also standarizes variable names for each specific value.
    """
    column_keys = {
        'city': {
            'name': 'name',
            'code': 'l10n_mx_edi_code',
            'external_id': 'external_id',
            'state_name': 'state_name',
            'state_external_id': 'state_external_id',
            'country_external_id': 'country_external_id',
        },
        'colony': {
            'name': 'd_asenta',
            'code': 'id_asenta_cpcons',
        },
        'zipcode': {
            'name': 'd_codigo',
        },
        'mx_record': {
            'state_name': 'd_estado',
            'city_code': 'c_mnpio',
        },
        'ccp': {
            'colony_code': 'c_Colonia',
            'zip_code': 'c_CodigoPostal',
            'colony_name': 'asentamiento',
        }

    }

    """
    The inputs consist of 3 files:

    File 1: Contains data regarding cities, also referred to as municipalities. This file is exported from odoo and the column names manually changed on a spreadsheet editor.
    The columns include:
    - `code`: Unique identifier for each city inside a satate.
    - `name`: Name of the city.
    - `external_id`: External identifier for each city.
    - `state_name`: Name for the state in which the city is located.
    - `state_external_id`: External identifier for the state.
    - `country_external_id`: External identifier for the country.

    File 2: Contains information on colonies and zip codes. This file is obtained from the official source.
    The columns are:
    - `d_asenta`: Name of the colony.
    - `id_asenta_cpcons`: Identifier for each colony. It's unique only for each city, it goes from 001 onwards and each city starts over again the count at 001 for their colonies.
    - `state_name`: Official name of the state with full spaces and grammar.
    - `c_mnpio`: Identifier for the city to which the colony belongs.
    - `d_codigo`: Zip code associated with the colony.

    File 3: Contains the the list of colonies and its assigned zip code and colony code from the official and upto date dataset on the website from the SAT.
    The columns are:
    - `c_Colonia`: Name of the colony.
    - `c_CodigoPostal`: code associated with the colony.
    - `asentamiento`: Name of the colony.

    Additionally, zip codes are extracted from the same file as colonies, using the provided column keys.

    This script will not generate new cities (municipalities) found on File 2. It'll leave those record cells empty (or maybe it'll crash, you let me know).
    I'd recommend to write a script that checks for this and generates an ods file to import newly found cities into odoo, then export them and resume using this script.
    """

    cwd = os.getcwd()

    # Inputs
    cities_file_path = os.path.join(cwd, "res_city.ods")
    correos_de_mexico_file_path = os.path.join(cwd, "correos_de_mexico.ods")
    ccp_file_path = os.path.join(cwd, "carta_porte_30.ods")

    # Outputs
    colony_output_file_path = os.path.join(cwd, "res_colony.ods")
    zip_output_file_path = os.path.join(cwd, "res_zip.ods")
    error_logs_file_path = os.path.join(cwd, "errors.log")

    process_directory(
        cities_file_path=cities_file_path,
        correos_de_mexico_file_path=correos_de_mexico_file_path,
        colony_output_file_path=colony_output_file_path,
        zip_output_file_path=zip_output_file_path,
        error_logs_file_path=error_logs_file_path,
        column_keys=column_keys
    )
